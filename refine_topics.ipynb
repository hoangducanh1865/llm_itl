{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2abe8b6-b3af-413c-8f8f-425e5dd3b3bd",
   "metadata": {},
   "source": [
    "# Refine topics and get confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896bd7d-c0fc-49fc-aedc-45c874d7fdc9",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to refine topics using a large language model (LLM) and obtain confidence scores for the refinements. For more details about our approach, please refer to our [Paper](https://arxiv.org/abs/2411.08534).\n",
    "\n",
    "Ensure that your machine has a GPU to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b20ed44d521f9c5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T00:48:02.709130Z",
     "start_time": "2025-05-28T00:48:02.707876Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntuadmin/miniconda3/envs/LLM-ITL/lib/python3.9/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/ubuntuadmin/miniconda3/envs/LLM-ITL/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from generate import generate_one_pass, generate_two_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c66fc-c215-4e26-a13a-ab2237016d91",
   "metadata": {},
   "source": [
    "We support the following LLMs. Please follow the links below to gain access (if necessary) to the corresponding models:\n",
    "\n",
    "- Llama-3-8B-Instruct -- [model link](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "- Llama-3-70B-Instruct -- [model link](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)\n",
    "- Mistral-7B-Instruct-v0.3 -- [model link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n",
    "- Yi-1.5-9B-Chat -- [model link](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)\n",
    "- Phi-3-mini-128k-instruct -- [model link](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)\n",
    "- Qwen1.5-32B-Chat -- [model link](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)\n",
    "\n",
    "\n",
    "We are not limited to these LLMs. Feel free to play with other models and modify the prompts in the ``create_messages_xx`` functions within ``generate.py``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9226f84bc9e6006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the LLM\n",
    "\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "# model_name = '01-ai/Yi-1.5-9B-Chat'\n",
    "# model_name = 'microsoft/Phi-3-mini-128k-instruct'\n",
    "\n",
    "# Larger models:\n",
    "# model_name = 'Qwen/Qwen1.5-32B-Chat'\n",
    "# model_name = 'meta-llama/Meta-Llama-3-70B-Instruct'\n",
    "\n",
    "# load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16\n",
    "                                             ).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257d8457-70de-4aa6-b2e3-3b37f2f42a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example topics\n",
    "topic1 = ['book', 'university', 'bank', 'science', 'vote', 'gordon', 'surrender', 'intellect', 'skepticism', 'shameful']\n",
    "topic2 = ['game', 'team', 'hockey', 'player', 'season', 'year', 'league', 'nhl', 'playoff', 'fan']\n",
    "topic3 = ['written', 'performance', 'creation', 'picture', 'chosen', 'clarify', 'second', 'appreciated', 'position', 'card']\n",
    "topics = [topic1, topic2, topic3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8d1688-399d-4416-b69a-e80df6cd0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configurations\n",
    "voc = None                        # A list of words. \n",
    "                                  # The refined words will be filtered to retain only those that are present in the vocabulary.\n",
    "\n",
    "inference_bs = 5                  # Batch size: the number of topics sent to the LLM for refinement at once.\n",
    "                                  # Increase or reduce this number depending on your GPU memory.\n",
    "\n",
    "\n",
    "instruction_type = 'refine_labelTokenProbs'    \n",
    "\n",
    "# Different ways to get confidence socre, we support the following options:\n",
    "# 'refine_labelTokenProbs'    -- Label token probaility\n",
    "# 'refine_wordIntrusion'      -- Word intrusion confidence\n",
    "# 'refine_askConf'            -- Ask for confidence\n",
    "# 'refine_seqLike'            -- Length normalized sequence likelihood\n",
    "# 'refine_twoStep_Score'      -- Self-reflective confidence score\n",
    "# 'refine_twoStep_Boolean'    -- p(True)\n",
    "\n",
    "# For more details about these confidence scores, please refer to our Paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c1e572-5191-4fa4-946f-0a499f1e14c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLM Feedback ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate topics\n",
    "if instruction_type in ['refine_labelTokenProbs', 'refine_wordIntrusion', 'refine_askConf', 'refine_seqLike']:\n",
    "    topic_probs, word_prob = generate_one_pass(model,\n",
    "                                               tokenizer,\n",
    "                                               topics,\n",
    "                                               voc=voc,\n",
    "                                               batch_size = inference_bs,\n",
    "                                               instruction_type=instruction_type)\n",
    "\n",
    "elif instruction_type in ['refine_twoStep_Score', 'refine_twoStep_Boolean']:\n",
    "    topic_probs, word_prob = generate_two_step(model,\n",
    "                                                   tokenizer,\n",
    "                                                   topics,\n",
    "                                                   voc=voc,\n",
    "                                                   batch_size=inference_bs,\n",
    "                                                   instruction_type=instruction_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "954fd6b6-2525-4d66-bdbe-a98e00964fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic label and confidence:\n",
      "Topic 0:  {'Higher Learning': 0.17292044166298481}\n",
      "Topic 1:  {'Ice Sport': 0.39517293597115355}\n",
      "Topic 2:  {'Artistic Expression': 0.056777404880380314}\n",
      "\n",
      "Topic words and probabilities:\n",
      "Topic 0:  {'university': 0.1, 'degrees': 0.1, 'curriculum': 0.1, 'book': 0.1, 'research': 0.1, 'skepticism': 0.1, 'education': 0.1, 'intellect': 0.1, 'knowledge': 0.1, 'science': 0.1}\n",
      "Topic 1:  {'nhl': 0.1, 'league': 0.1, 'season': 0.1, 'hockey': 0.1, 'match': 0.1, 'player': 0.1, 'rival': 0.1, 'playoff': 0.1, 'game': 0.1, 'team': 0.1}\n",
      "Topic 2:  {'creative': 0.1, 'written': 0.1, 'picture': 0.1, 'appreciated': 0.1, 'artist': 0.1, 'imagination': 0.1, 'clarify': 0.1, 'creation': 0.1, 'chosen': 0.1, 'performance': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print('Topic label and confidence:')\n",
    "for i in range(len(topic_probs)):\n",
    "    print('Topic %s: ' % i, topic_probs[i])\n",
    "\n",
    "print()\n",
    "print('Topic words and probabilities:')\n",
    "for i in range(len(word_prob)):\n",
    "    print('Topic %s: ' % i, word_prob[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
